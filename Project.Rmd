---
title: "MAP 535 - Data Analysis Project"
author: "Adrien Toulouse & Paul-Antoine Girard"
output: pdf_document
---

```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(corrplot)
library(ggplot2)
library(car)
library(lasso2)
library(pls)
library(caret)
library(glmnet)
```

## Introduction

Our task is to analyse the dataset named **House Prices: Advanced Regression Techniques**. It contains the sale price of about 1500 residential homes located in Ames, Iowa, along with 79 explanatory variables describing (almost) every aspect of the houses. The dataset has already been preprocessed to deal with missing values, so we will work on a reduced dataset containing 68 variables.

Our aim within this project is to focus on dimensionality reduction by doing variable selection. Variable selection can be defined as selecting a subset of the most relevant features.
The objectives of feature selection include: building simpler and more comprehensible models, improving performance, and preparing clean, understandable data. Indeed, with a large number of features, learning models tend to overfit which may cause performance degradation on unseen data.

We can therefore adress the following question: **What are the most relevant features to explain the sale price of houses of the dataset?**

To answer this question we will first analyse the variables and assess their relevance by looking at their correlation with the regression target: *SalePrice*. We will also build and compare several linear regression models with different number of variables and finally conclude on the relevance of the features. Our work will be focused on finding the best linear prediction model using a minimum number of variables. We can therefore state our research hypothesis as follows: **Can we construct a performant linear regression model by selecting only the most appropriate variables? How does it compare to larger or other models?**


## Exploratory Data Analysis

```{r imports}
trainImputed <- read.csv(file='train_imputed.csv')
trainPP <- read.csv(file='train_preprocessed.csv')
```

### 1. Transformations

We start by looking at the data to see how it is structured.  
We find that the *SalePrice* has skewness in its distribution. So, a first transformation we do is to take the log of the house prices to reduce the effect of the tail in the density of our response variable.

```{r log transformation}
par(mfrow=c(2,1))
plot(density(trainImputed$SalePrice), main='SalePrice density', sub='', xlab='', ylab='')
trainImputed$LogSalePrice <- log(trainImputed$SalePrice)
trainImputed <- select(trainImputed, -c("SalePrice", "X")) #X is unrelated to our study
plot(density(trainImputed$LogSalePrice), main='Log SalePrice density', sub='', xlab='', ylab='')
```

We do the same transformation for the variables LotArea, TotalBsmtSF and GrLivArea from our first analysis.

```{r log transformation 2}
trainImputed$LogLotArea <- log(trainImputed$LotArea)
trainImputed <- select(trainImputed, -c("LotArea"))
trainImputed$LogTotalBsmtSF <- log(trainImputed$TotalBsmtSF)
trainImputed <- select(trainImputed, -c("TotalBsmtSF"))
trainImputed$LogGrLivArea <- log(trainImputed$GrLivArea)
trainImputed <- select(trainImputed, -c("GrLivArea"))
```

### 2. Numeric variables

Looking at the numeric variables, we check correlation between the different variables together and then the correlation of SalePrice with the variables.
The first step is not useful when you try to predict a variable. It is however very important when you try to see which variables are the most important to explain price.  
To select regressors, it is important to analyze the correlation between numerical variables, since there might be multicollinearity problems, and we can create clusters of variables based on correlations. Indeed we know that having correlated response variables is not efficient in linear models and we also want to create a reduced model.  

```{r correlation}
#var.numeric <- colnames(trainPP)[sapply(trainPP, is.numeric)]
#
#trainPP %>%
#  select(var.numeric) %>%
#  cor() %>%
#  corrplot(method = 'color', order = "hclust", tl.pos = 'n') %>%
#  heatmap (symm=T)
```

Some variables like for example YearBuilt and YearRemodAdd are very strongly correlated and we will not need to keep both variables.

```{r plot 1}
ggplot(trainImputed, aes(YearBuilt, YearRemodAdd)) + geom_point() + geom_smooth(method=lm)
```

Let's now look at the correlations with our response variables to see which variables explain the house sale price well.

```{r plot 2}
ggplot(trainImputed, aes(OverallQual, LogSalePrice)) + geom_point() + geom_smooth(method=lm)
```


```{r plot 3}
ggplot(trainImputed, aes(LogTotalBsmtSF, LogSalePrice)) + geom_point() + geom_smooth(method=lm)
```

For example, Overall quality and LogTotalBsmtSF impact Sale Price as the scatter plots show what looks like a linear relationship. By plotting other scatter plots, we found that the variables YearBuilt, LogLotArea, OverallCond are also strongly correlated with the price. Including these predictors in our model should be a good idea.

The regressor *Overallqual* is particularly interesting as it rates the overall material and finish of the house. It is a kind of summary of other variables, so it can help us to reduce the size of our model as it explains well the price.

High positive correlation with Sale Price:

OverallQual
YearBuilt
YearRemodAdd
MasvnrArea
BsmtFinSF1
TotalBsmtSF
1stFlrSF
GrLiveArea

### 3. Factor variables

Concerning categorical variables, a variable may need to be considered in our model if it has different boxplots for each category when considering the *SalePrice*, as this will indicate a clear dependency between the two variables.

```{r plot 4}
ggplot(data = trainImputed) +
  geom_boxplot(aes(y=LogSalePrice, x = MSZoning))
```

We remark that the variable *MSZoning* corresponds to this situation. Indeed, these boxplots are quite different visually from one another which indicates clearly that MSZoning is an important variable to explain the Sale Price.

```{r Anova}
res.aov <- aov(LogSalePrice ~ MSZoning, data = trainImputed)
summary(res.aov)
```

This is confirmed by the one way Anova test as the p-value is less than the significance level 0.05. We can conclude that there are significant differences between the MSZoning categories when considering the Sale Price and make us say that we should include MSZoning in our model.

```{r plot 5}
ggplot(data = trainImputed) +
  geom_boxplot(aes(y=LogSalePrice, x = GarageQual))
```

```{r Anova 2}
res.aov <- aov(LogSalePrice ~ GarageQual, data = trainImputed)
summary(res.aov)
```

We find similar results when looking at the GarageQual variable. However, the boxplots do not completely follow intuition. Indeed the boxplots indicate that on average houses with garages in excellent quality have a lower sale price that garages in good quality. This seems to indicate that garage quality is not a key variable when trying to explain the sale price of a house.

Some categorical variables like for example RoofMatl or Condition2 are not very interesting to explain sale price as they are too heavily unbalanced.
```{r}
table(trainImputed$RoofMatl)
table(trainImputed$Condition2)
```


### 4. Ancova analysis

Now we plot two quantitative variables together with one factor variable.

```{r Ancova}
p <- ggplot(trainImputed, aes(y=LogSalePrice, x=LogGrLivArea, color = MSZoning, fill = MSZoning)) +
  geom_point() + theme_classic()
p + stat_smooth(method = "lm")
```

This helps to make clear that while GrLiveArea has a large predictive effect for LogSalePrice (the slopes of all the lines are clearly non-zero), there is also an effect of group assignment: for example the houses assigned to the FV MSZoning have a higher Sale Price than the houses assigned to RH.  


### 5. Key findings from EDA

To sum up, our findings from this first part are the following (we have not displayed all our graphs for space issues):
 * we will use a log transformation on the sale price to reduce the impact of the tail in its distribution
 * the variables YearBuilt and YearRemodAdd as well as the variables LotArea and LotFrontage are highly correlated two by two.
 * the numeric variables LogTotalBsmtSF, LogGrLivArea, OverallQual, OverallCond, LogLotArea are interesting when explaining Sale Price because they are highly correlated with our response variables
 * the factor variables MSZoning, CentralAir, BsmtQual, KitchenQual are interesting when explaining Sale Price because of the large difference in each category boxplots. The factor variable Roofmatl is not very interesting because it is highly unbalanced (almost all the observations take the value Compshg and we have only one observation for some of the other roof material types).  
Moreover, we have confirmed our intuition with statistical tests and have ploted numeric and factor variables together with Ancova plots.

Now that we have more information on our data, let's try to build multiple linear models. We will start from the full model and then use our findings from this exploratory analysis part to select variables and build other models.


## Modeling and Diagnostics

In this part, we are going to build different linear regression models and analyse their differences to select the one that we think fits the best our research hypothesis.

### 1. Full model

As we did at the beginning, we need to take the log of the variable *SalePrice*.

```{r log transformation 3}
trainPP$LogSalePrice <- log(trainPP$SalePrice)
trainPP <- select(trainPP, -c("SalePrice", "X")) #X is not related to our study
```

First, we start by doing a linear regression with all the variables of the dataset.

```{r full lm}
full_model = lm(LogSalePrice ~ ., data = trainPP)
summary(full_model)
#summary(full_model)$coefficient[,4]
AIC(full_model)
```

As we said in the introduction, our interest in this study is to select variables that explain the best our model. Therefore, we are going to use the Student tests statistics realized for each variable. Some of the variables are irrelevant if we consider the p-values related to this test. At a significance level of 0.001, this technique suggests us to only keep the following variables: MSZoning, LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd, RoofMatl, TotalBsmtSF, CentralAir, GrLivArea, KitchenQual, Fireplaces, GarageQual. Concerning the qualitative variables, we decide to only keep the one that have many categories that are relevant for the model at a significance level of 0.001, and not only one category (that is the case for the followings: Condition1, Condition2, Heating, Functional).  

Overall, this full model has a $R^2$ coefficient of 0.94 (can't be improve since we can't add new variables), a $R^2_a$ of 0.93, and a AIC of -2314.3. The F test statistic yields a very low p-value, that shows that the model is meaningful at a level of 0.05.  
Now, we need to see if the residuals verify the postulates neccessary to valid the linear model.

```{r postulates 1}
par(mfrow=c(2,2))
plot(full_model)
```

```{r tests 1}
acf(residuals(full_model), main='Auto-correlation plot')
ncvTest(full_model)
durbinWatsonTest(full_model)
shapiro.test(residuals(full_model))
```

The residuals seem to have a mean of zero, and they are uncorrelated. However, the others assumptions aren't verified. The Breush-Pagan, and Shapiro-Wlik tests give both of them a p-value that is lower than 0.05. So, the hypotheses of homoscedastic variance, and gaussian distribution are rejected at a significance level of 5%. Finally, none of the residuals on the last plot have a cook distance larger than 0.5, but R tells us that it didn't plot few points that have a leverage of one. This means that the fitted value corresponds exactly to the observed value. Since we have many regressors in this model, certain combinations of modalities are associated to only 1 observation or some factors contain the same information, and that lead to the problem of leverage one. Therefore, we need to work on our selection of variables to reduce the number of regressors, and define a reduced model that verify the assumptions needed for its validity.

```{r outliers 1}
outlierTest(full_model)
```

### 2. Model using the forward method

We now use methods based on minimizing the AIC to automatically select a reduced number of variables for our model. For the forward method, at each step the model adds the variable which reduces the most the criterion, and stops when adding a variable doesn't improve the accuracy of the model.

```{r forward}
reg0=lm(LogSalePrice~1,data=trainPP)
select.variables.forward = step(reg0, LogSalePrice~MSSubClass+MSZoning+LotFrontage+LotArea+Street+LotShape+LandContour+Utilities+LotConfig+LandSlope+Neighborhood+Condition1+Condition2+BldgType+HouseStyle+OverallQual+OverallCond+YearBuilt+YearRemodAdd+RoofStyle+RoofMatl+Exterior1st+Exterior2nd+MasVnrType+MasVnrArea+ExterQual+ExterCond+Foundation+BsmtQual+BsmtCond+BsmtExposure+BsmtFinType1+BsmtFinSF1+BsmtFinType2+BsmtUnfSF+TotalBsmtSF+Heating+HeatingQC+CentralAir+Electrical+X1stFlrSF+X2ndFlrSF+GrLivArea+BsmtFullBath+BsmtHalfBath+FullBath+HalfBath+BedroomAbvGr+KitchenQual+TotRmsAbvGrd+Functional+Fireplaces+GarageType+GarageYrBlt+GarageFinish+GarageCars+GarageArea+GarageQual+GarageCond+PavedDrive+WoodDeckSF+OpenPorchSF+MoSold+YrSold+SaleType+SaleCondition, data=trainPP, trace=FALSE, direction=c('forward'))
summary(select.variables.forward)
```

```{r both}
reg0=lm(LogSalePrice~1,data=trainPP)
select.variables.both = step(reg0, LogSalePrice~MSSubClass+MSZoning+LotFrontage+LotArea+Street+LotShape+LandContour+Utilities+LotConfig+LandSlope+Neighborhood+Condition1+Condition2+BldgType+HouseStyle+OverallQual+OverallCond+YearBuilt+YearRemodAdd+RoofStyle+RoofMatl+Exterior1st+Exterior2nd+MasVnrType+MasVnrArea+ExterQual+ExterCond+Foundation+BsmtQual+BsmtCond+BsmtExposure+BsmtFinType1+BsmtFinSF1+BsmtFinType2+BsmtUnfSF+TotalBsmtSF+Heating+HeatingQC+CentralAir+Electrical+X1stFlrSF+X2ndFlrSF+GrLivArea+BsmtFullBath+BsmtHalfBath+FullBath+HalfBath+BedroomAbvGr+KitchenQual+TotRmsAbvGrd+Functional+Fireplaces+GarageType+GarageYrBlt+GarageFinish+GarageCars+GarageArea+GarageQual+GarageCond+PavedDrive+WoodDeckSF+OpenPorchSF+MoSold+YrSold+SaleType+SaleCondition, data=trainPP, trace=FALSE, direction=c('both'))
summary(select.variables.both)
```

```{r backward}
select.variables.backward = step(full_model, scope= ~1, direction=c("backward"), trace=FALSE)
summary(select.variables.backward)
```

```{r AICs}
AIC(select.variables.forward)
AIC(select.variables.both)
AIC(select.variables.backward)
```

These three methods lead to the same model that has an AIC value of -2382.3. If we take this criterion as selection, this model is better than the full model, and also it selects less variables. The F-test gives a low p-value, so it is meaningful. Finally, if we look at the Student tests realized, we can observe that we found the same variables as before, but we can also add the two following variables: BsmtQual, BsmtFinSF1.

Now, let's take a look at the residuals. We need to see if the reduction of the number of variables helps us to find a linear model that verify the assumptions needed for the validity, and if we reduced the number of observations with leverage one.

```{r postulates 2}
par(mfrow=c(2,2))
plot(select.variables.forward)
```

```{r tests 2}
acf(residuals(select.variables.forward), main='Auto-correlation plot')
ncvTest(select.variables.forward)
durbinWatsonTest(select.variables.forward)
shapiro.test(residuals(select.variables.forward))
```

The residuals still don't verify the assumptions needed for the validity of our model. As before, the mean seem to be zero, but the others assumptions aren't verified. The Breush-Pagan, Durbin-Watson, and Shapiro-Wilk tests give all of them a p-value that is lower than 0.05. Also, there is still observations that aren't plot on the last plot because they have a leverage of one. So we need to reduce even more the number of variables and to use our work in the first to see which observations lead to this problem.

### 3. Reduced model based on the Student tests and our EDA work

In this part, we are going to construct a model based on the regressors selected by the p-values of the Student tests realized for each variable in the linear regression model obtained by the forward method, and by our work done in the first part of the study.  

In order to obtain a model that verify the assumptions needed in a linear model, we first wanted to use the transformations made in the first part, but how the variables have been encoded don't let us apply the log transformations (some values are now negative).  

Our base model contains all the following variables selected in the part II: MSZoning, LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd, RoofMatl, TotalBsmtSF, CentralAir, GrLivArea, KitchenQual, Fireplaces, GarageQual, BsmtQual, BsmtFinSF1. From there we decided to remove the variables YearRemodAdd, RoofMatl, CentralAir, KitchenQual, BsmtFinSF1, GarageQual and to add the Neighborhood, GarageCars.

Indeed, as we saw in our previous analysis, the numeric variables TotalBsmtSF, GrLivArea, OverallQual, OverallCond, LotArea are interesting when explaining Sale Price because they are highly correlated with our response variables. The factor variables MSZoning, Neighborhood, GarageCars and Fireplaces are interesting when explaining Sale Price because of the large difference in each category boxplots. 
We removed the factor variable Roofmatl which is not very interesting because it is highly unbalanced and the variable YearRemodAdd because of its high correlation with YearBuilt.  

Our reduced model is finally composed of: MSZoning + LotArea + OverallQual + OverallCond + YearBuilt + TotalBsmtSF + GrLivArea + Neighborhood + GarageCars + Fireplaces.

Let's train the model with our selected variables to see how the postulates are verified and if there are outliers, and observations with leverage one.
    
```{r reduced_model}
reduced_model = lm(LogSalePrice ~ MSZoning + LotArea + OverallQual + OverallCond + YearBuilt + TotalBsmtSF + GrLivArea + Neighborhood + GarageCars + Fireplaces, data = trainPP[-c(524, 1299),])
summary(reduced_model)

AIC(reduced_model)
```

We can observe that most of the variables still have a p-value for the Student tests that are smaller than 0.01, except for some of the categories of the variable *Neighborhood*.
The F-statistic yields that the model is meaningful, but the AIC is larger than before, and the $R^2$, $R^2_a$ coefficients are smaller.

```{r postulates 3}
par(mfrow=c(2,2))
plot(reduced_model)
```

```{r tests 3}
acf(residuals(reduced_model), main='Auto-correlation plot')
ncvTest(reduced_model)
durbinWatsonTest(reduced_model)
shapiro.test(residuals(reduced_model))
```

```{r outliers 3}
outlierTest(reduced_model)
influenceIndexPlot(reduced_model)
```

Looking at the assumptions, the residuals the gaussian assumption is still not validated (we could'nt do some log transformations) but the others seem validated. Looking at the ncvTest we realize that the p-value is still lower than the 5% treshold but has been considerably improved compared to previous models and the Scale Location plot doesn't really show any special tendencies.

### 4. Lasso Model

Finally, we do a Lasso regression which does automatic variable selection by putting some of the coefficients to zero. In fact, adding the $L^1$ penalization induces sparsity in the estimator.

```{r lasso}
lasso=train(LogSalePrice~., trainPP, method='glmnet', tuneGrid=expand.grid(alpha=1,lambda=seq(0.01,0.1,length=10)))
lasso$bestTune
```

```{r lasso important features}
varImp(lasso,scale=F)
```

The lasso regression gives us the variables that explain the most its predictions. We can observe that this method uses mainly the following regressors: Condition2, GrLivArea, OverallQual, Neighborhood, Functional, CentralAir, YearBuilt, GarageCars, LotArea, SaleType, OverallCond, RoofMatl, Condition1, BsmtFinSF1.  
We used some of these features in our reduced model, lasso is less selective than our model and used a lot more regressors. Therefore, it will be interesting to see how the two different models predict, and to compare their respective RMSE.

## RMSE

In order to use the train-test split already done, we need to modify these two dataset as we have done in our study. 

```{r modifications train/test}
train <- train[ !(train$Id %in% c(1299,524)), ]
train$LogSalePrice <- log(train$SalePrice)
test$LogSalePrice <- log(test$SalePrice)
train <- select(train, -c("SalePrice", "Id"))
test<- select(test, -c("SalePrice", "Id"))
Ytrain = train$LogSalePrice
Ytest = test$LogSalePrice
```

```{r predictions lasso}
lasso=train(LogSalePrice~., train, method='glmnet', tuneGrid=expand.grid(alpha=1,lambda=0.01))
lasso
```

```{r prediction reduced_model}
reduced_model = lm(LogSalePrice ~ MSZoning + LotArea + OverallQual + OverallCond + YearBuilt + TotalBsmtSF + GrLivArea + Neighborhood + GarageCars + Fireplaces, data = trainPP[-c(524, 1299),])
RMSE_lm_test=sqrt(mean((predict(reduced_model, newdata = test)-Ytest)^2))
print(RMSE_lm_test)
```
 
 RMSE obtained by lasso: 0.127
 
 RMSE obtained by our reduced model: 0.124

## Conclusion

Our analysis now allows us to provide answers to our initial questions. We have found which variables explained the most Sale price and built a reduced model using a minimum number of variables. 
Our reduced model is as good as the lasso model when trying to predict Sale Price (similar RMSE). However, it has the advantage to use a lot less variables which makes it faster and more easily explainable.
