---
title: "MAP 535 - Data Analysis Project"
author: "Adrien Toulouse & Paul-Antoine Girard"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
library(dplyr)
library(corrplot)
library(ggplot2)
library(car)
```

## Introduction

Write a short introduction describing the research problem. Clearly state the research hypothesis at the end.

Our task is to analyse a dataset, named **House Prices: Advanced Regression Techniques**. It contains our response variable, the sale price of about 1500 residential homes located in Ames, Iowa, along with 79 explanatory variables describing (almost) every aspect of the houses. The dataset has already been preprocessed to deal with missing values, so we will work on a reduced dataset containing 68 variables.

Our work will be focused on finding the best linear prediction model using a minimum number of variables. We can therefore state our research hypothesis as follows: **Can we construct a performant linear regression model by selecting only the 10 most appropriate variables? How does it compare to larger models?** 

To test our hypothesis, we will start by describing the data and apply descriptive statistics to better apprehend it and preprocess the variables if necessary. We will then try to select the most important variables and after checking if the linear model assumptions are verified, we will build multiple linear regression models and compare them.

## Exploratory Data Analysis

We start by uploading our data.
```{r imports}
trainImputed <- read.csv(file='train_imputed.csv')
trainPP <- read.csv (file='train_preprocessed.csv')
```

A first transformation we do is to take the log of the house prices to reduce the effect of the tail in the density of our response variable. 

```{r log transformation}
par(mfrow=c(2,1)) 
plot(density(trainImputed$SalePrice))
trainImputed$LogSalePrice <- log(trainImputed$SalePrice)
trainImputed <- select(trainImputed, -c("SalePrice", "X"))
trainPP$LogSalePrice <- log(trainPP$SalePrice)
trainPP <- select(trainPP, -c("SalePrice", "X")) #X is unrelated to our study
plot(density(trainImputed$LogSalePrice))
```

By ploting scatter plots, we find that some variables are strongly correlated with the Sale Price. Such variables are interesting to keep in a linear model as they explain well the price. 

```{r}
ggplot(trainImputed, aes(OverallQual, LogSalePrice)) + geom_point() + geom_smooth()
```

We can also plot boxplots for categorical variables. We are looking for variables that have different boxplots in each category when considering the Sale Price as this will indicate a clear dependency between the two variables. 

```{r}
ggplot(data = trainImputed) + 
  geom_boxplot(aes(y=LogSalePrice, x = MSZoning))
```

This first analysis visually indicates clearly that MSZoning is an important variable to explain the Sale Price.

```{r}
res.aov <- aov(LogSalePrice ~ MSZoning, data = trainImputed)
summary(res.aov)
```

This is confirmed by the one way Anova test as the p-value is less than the significance level 0.05. We can conclude that there are significant differences between the MSZoning caregories when considering the Sale Price and make us say that we should include MSZoning in our model.

```{r}
ggplot(data = trainImputed) + 
  geom_boxplot(aes(y=LogSalePrice, x = GarageQual))
```

```{r}
res.aov <- aov(LogSalePrice ~ GarageQual, data = trainImputed)
summary(res.aov)
```

We find similar results when looking at the GarageQual variable. However, the boxplots do not completely follow intuition. Indeed the boxplots indicate that on average houses with garages in excellent quality have a lower sale price that garages in good quality. This seems to indicate that garage quality is not a key variable when trying to explain the sale price of a house.

To select regressors, it is important to also analyze the correlation between numerical variables and to create clusters of variables based on correlations. Indeed we know that having correlated response variables is not efficient in linear models. This will help us know which variables contain redundant information and which variables to keep. 

```{r correlations}
var.numeric <- colnames(trainImputed)[sapply(trainImputed, is.numeric)]

trainImputed %>%
  select(var.numeric) %>%
  cor() %>%
  corrplot(method = 'color', order = "hclust", tl.pos = 'n') %>%
  heatmap (symm=T)

## Don't know how to print only the second graph
```

Now that we have more information on our data, let's try to build multiple linear models. We will use what we found in this exploratory analysis part to select variables and build these different models. 

## Modeling and Diagnostics

In this part, we are going to build different linear regression models and analyse their differences to select the one that we think fits the best our research hypothesis.

First, we start by doing a linear regression with all the variables of the dataset. 

```{r}
price_lm = lm(LogSalePrice ~ ., data = trainPP)
summary(price_lm)
```

The results of this model show us that some of the variables are irrelevant (by looking at the pvalue of the test realized for each variables.

We then use a method based on minimizing the AIC to automatically select a reduced number of variables for our model.

```{r}
select.variables.backward = step(price_lm,scope= ~1,direction="backward",trace=FALSE) 
summary(select.variables.backward)
```

This model still selects a lot of variables. Let's try to reduce again the number of regressors based on the p_values obtained for the different regressors and what we discovered in the previous exploratory data analysis.

The summary of the previous model indicated that the following variables seem important as they obtain the lowest p_values: OverallQual, OverallCond, YearBuilt, YearRemodAdd, RoofMatl, LotArea, MSZoning, Fireplaces, GrLivArea, BsmtQual, TotalBsmtSF, KitchenQual, GarageQual.

Let's confirm

## Final model and prediction



## Conclusion


