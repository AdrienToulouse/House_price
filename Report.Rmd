---
title: "MAP 535 - Data Analysis Project - Report"
author: "Adrien Toulouse & Paul-Antoine Girard"
output: pdf_document
---

```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library(dplyr)
library(corrplot)
library(ggplot2)
library(car)
library(lasso2)
```

## Introduction

Our task is to analyse the dataset named **House Prices: Advanced Regression Techniques**. It contains our response variable, the sale price of about 1500 residential homes located in Ames, Iowa, along with 79 explanatory variables describing (almost) every aspect of the houses. The dataset has already been preprocessed to deal with missing values, so we will work on a reduced dataset containing 68 variables.

Our aim within this project is to focus on dimensionality reduction by doing variable selection. Variable selection can be defined as selecting a subset of the most relevant features. 
The objectives of feature selection include: building simpler and more comprehensible models, improving performance, and preparing clean, understandable data. Indeed, with a large number of features, learning models tend to overfit which may cause performance degradation on unseen data. Moreover, data of high dimensionality can significantly increase the memory storage requirements and computational costs for data analytics.

We can therefore adress the following question: **What are the most relevant features to explain the sale price of houses in our dataset?** 

To answer our question we will first analyse the variables and assess their relevance by looking at the correlation with the regression target: *SalePrice*. We will also build and compare several linear regression models with different number of variables and finally conclude on the relevance of the features. Our work will be focused on finding the best linear prediction model using a minimum number of variables. We can therefore state our research hypothesis as follows: **Can we construct a performant linear regression model by selecting only the most appropriate variables? How does it compare to larger or other models?** 


## Exploratory Data Analysis

```{r imports, message=FALSE, warning=FALSE, include=FALSE}
trainImputed <- read.csv(file='train_imputed.csv')
trainPP <- read.csv (file='train_preprocessed.csv')
```

### 1. Transformations

We start by looking at the data to see how it is structured.  
We find that the *SalePrice* has skewness in its distribution. So, a first transformation we do is to take the log of the house prices to reduce the effect of the tail in the density of our response variable. We do the same transformation for the variables LotArea, TotalBsmtSF and GrLivArea.

```{r log transformation, message=FALSE, warning=FALSE, echo=FALSE}
par(mfrow=c(2,1)) 
plot(density(trainImputed$SalePrice), main='SalePrice density', sub='', xlab='', ylab='')
trainImputed$LogSalePrice <- log(trainImputed$SalePrice)
trainImputed <- select(trainImputed, -c("SalePrice", "X")) #X is unrelated to our study
plot(density(trainImputed$LogSalePrice), main='Log SalePrice density', sub='', xlab='', ylab='')
trainImputed$LogLotArea <- log(trainImputed$LotArea)
trainImputed <- select(trainImputed, -c("LotArea"))
trainImputed$LogTotalBsmtSF <- log(trainImputed$TotalBsmtSF)
trainImputed <- select(trainImputed, -c("TotalBsmtSF"))
trainImputed$LogGrLivArea <- log(trainImputed$GrLivArea)
trainImputed <- select(trainImputed, -c("GrLivArea"))
```

### 2. Numeric variables

Looking at the numeric variables, we check correlation between the different variables together and then the correlation of SalePrice with the variables.
The first step is not useful when you try to predict a variable. It is however very important when you try to see which variables are the most important to explain price.  
To select regressors, it is important to analyze the correlation between numerical variables, since there might be multicollinearity problems, and we can create clusters of variables based on correlations. Indeed we know that having correlated response variables is not efficient in linear models and we also want to create a reduced model.  

```{r correlations, message=FALSE, warning=FALSE, echo=FALSE}
var.numeric <- colnames(trainPP)[sapply(trainPP, is.numeric)]

trainPP %>%
  select(var.numeric) %>%
  cor() %>%
  corrplot(method = 'color', order = "hclust", tl.pos = 'n') %>%
  heatmap (symm=T)
```

Some variables like for example YearBuilt and YearRemodAdd are very strongly correlated and we will not need to keep both variables.

```{r correlation 1, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(trainImputed, aes(YearBuilt, YearRemodAdd)) + geom_point() + geom_smooth(method=lm)
```

Let's now look at the correlations with our response variables to see which variables explain the house sale price well. For example, Overall quality and LogTotalBsmtSF impact Sale Price as the scatter plots show what looks like a linear relationship. By plotting other scatter plots, we found that the variables YearBuilt, LogLotArea, OverallCond are also strongly correlated with the price. Including these predictors in our model should be a good idea. 

```{r correlation 2, message=FALSE, warning=FALSE, echo=FALSE}
par(mfrow=c(2,1))
ggplot(trainImputed, aes(OverallQual, LogSalePrice)) + geom_point() + geom_smooth(method=lm)
ggplot(trainImputed, aes(LogTotalBsmtSF, LogSalePrice)) + geom_point() + geom_smooth(method=lm)
```

The regressor *Overallqual* is particularly interesting as it rates the overall material and finish of the house. It is a kind of summary of other variables, so it can help us to reduce the size of our model as it explains well the price.

### 3. Factor variables

Concerning categorical variables, a variable may need to be considered in our model if it has different boxplots for each category when considering the *SalePrice*, as this will indicate a clear dependency between the two variables.  
We remark that the variable *MSZoning* corresponds to this situation. Indeed, these boxplots are quite different visually from one another which indicates clearly that MSZoning is an important variable to explain the Sale Price.

```{r boxplot 1, message=FALSE, warning=FALSE, echo=FALSE}
ggplot(data = trainImputed) + 
  geom_boxplot(aes(y=LogSalePrice, x = MSZoning))
```

```{r Anova, message=FALSE, warning=FALSE, echo=FALSE}
res.aov <- aov(LogSalePrice ~ MSZoning, data = trainImputed)
summary(res.aov)
```

The one way Anova test confirmes what we thought, as the p-value is less than the significance level of 0.05. We can conclude that there are significant differences between the MSZoning categories when considering the Sale Price and make us say that we should include MSZoning in our model.

We find similar results when looking at the GarageQual variable. However, the boxplots do not completely follow intuition. Indeed the boxplots indicate that on average houses with garages in excellent quality have a lower sale price that garages in good quality. This seems to indicate that garage quality is not a key variable when trying to explain the sale price of a house.

By plotting other boxplots for the different variables, we found that the variables CentralAir, RoofMatl and Lot Config could also be good to explain the Sale Price.

### 4. Ancova analysis

Now we plot two quantitative variables together with one factor variable. 

```{r Ancova, message=FALSE, warning=FALSE, echo=FALSE}
p <- ggplot(trainImputed, aes(y=LogSalePrice, x=LogGrLivArea, color = MSZoning, fill = MSZoning)) + 
  geom_point() + theme_classic()
p + stat_smooth(method = "lm") 
```

This helps to make clear that while GrLiveArea has a large predictive effect for LogSalePrice (the slopes of all the lines are clearly non-zero), there is also an effect of group assignment: for example the houses assigned to the FV MSZoning have a higher Sale Price than the houses assigned to RH.  

### 5. Recap of our study

Now that we have more information on our data, let's try to build multiple linear models. We will use what we found in this exploratory analysis part to select variables and build our different models. 


## Modeling and Diagnostics

In this part, we are going to build different linear regression models and analyse their differences to select the one that we think fits the best our research hypothesis.

### 1. Full model

First, we start by doing a linear regression with all the variables of the dataset. 

```{r full lm, message=FALSE, warning=FALSE, include=FALSE}
full_model = lm(LogSalePrice ~ ., data = trainPP)
summary(full_model)
#summary(full_model)$coefficient[,4]
AIC(full_model)
```
